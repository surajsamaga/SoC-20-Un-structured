{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolo_preprocess_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVLO4M6-kTI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from keras.utils import Sequence\n",
        "import copy\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-JsuI35CSYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yolo_utils_new import BoundBox, box_iou"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSIMw2FRviQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageReader:\n",
        "  # Read images from svt dataset and preprocess/encode \n",
        "  def __init__(self, IMAGE_H, IMAGE_W):\n",
        "    '''\n",
        "    IMAGE_H, IMAGE_W - height, width of normalized image\n",
        "    '''\n",
        "    self.IMAGE_H = IMAGE_H\n",
        "    self.IMAGE_W = IMAGE_W\n",
        "\n",
        "  def encode(self, image):\n",
        "    image = cv2.resize(image, (self.IMAGE_H, self.IMAGE_W))\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = image/255\n",
        "    return(image)\n",
        "  \n",
        "  def fit(self, train_instance):\n",
        "    '''\n",
        "    read in and resize the image, annotations are resized accordingly\n",
        "        Input-\n",
        "        train_instance : dictionary containing filename, height, width and object\n",
        "        \n",
        "        {'filename': 'img/14_03.jpg',\n",
        "         'height' :   880,\n",
        "         'width'  :   1280,\n",
        "         'lex'    :\n",
        "         'object' : [{'label': 'room',\n",
        "                     'xmax': 318,\n",
        "                     'xmin': 284,\n",
        "                     'ymax': 184,\n",
        "                     'ymin': 100 }]\n",
        "        }\n",
        "        '''\n",
        "    if not isinstance(train_instance,dict):\n",
        "      train_instance = {'filename':train_instance}\n",
        "                \n",
        "    image_name = train_instance['filename']\n",
        "    image = cv2.imread(image_name)\n",
        "    h, w, c = image.shape\n",
        "\n",
        "    if image is None: \n",
        "      print('Cannot find ', image_name)\n",
        "\n",
        "    image = self.encode(image)\n",
        "    if \"object\" in train_instance.keys():\n",
        "      all_objs = copy.deepcopy(train_instance['object'])  # necessary, as annotations are being resized   \n",
        "\n",
        "    # fix object's position and size\n",
        "      for obj in all_objs:\n",
        "        for attr in ['xmin', 'xmax']:\n",
        "          obj[attr] = int(obj[attr] * float(self.IMAGE_W) / w)  # resize annotations\n",
        "          obj[attr] = max(min(obj[attr], self.IMAGE_W), 0)      # take care of boundary conditions\n",
        "\n",
        "        for attr in ['ymin', 'ymax']:\n",
        "          obj[attr] = int(obj[attr] * float(self.IMAGE_H) / h)\n",
        "          obj[attr] = max(min(obj[attr], self.IMAGE_H), 0)\n",
        "    else:\n",
        "      return image\n",
        "    return image, all_objs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNHOW-nQqEMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NEW(Sequence):\n",
        "  # Batch Generator, generates x_batch (np.array of images) and y_batch (np.array of labels)\n",
        "  # with augmentation\n",
        "  def __init__(self, images, config, shuffle = True, aug = True):\n",
        "    self.images = images # list of dicts with annotation data\n",
        "    self.config = config \n",
        "    self.shuffle = shuffle\n",
        "    self.aug = aug \n",
        "    self.anchors = [BoundBox(0, 0, config['ANCHORS'][2*i], config['ANCHORS'][2*i+1]) for i in range(self.config['N_ANCHORS'])]\n",
        "    # doesn't matter that encoding is done in min-max format. As long as its 0 centered or one of the points is 0, IOU remains unchanged.\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(np.ceil(float(len(self.images))/self.config['BATCH_SIZE']))\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    # shuffle at the end of epoch\n",
        "   if self.shuffle:\n",
        "    np.random.shuffle(self.images)\n",
        "\n",
        "  def load_image(self, i):\n",
        "    # loads the original image\n",
        "    return cv2.imread(self.images[i]['filename'])\n",
        "\n",
        "  def aug_image(self, train_instance, aug): \n",
        "    ### read image and augment by scaling and translating \n",
        "    image_name = train_instance['filename']\n",
        "    image = cv2.imread(image_name)\n",
        "    image = image.astype(np.float32)\n",
        "    if image is None: \n",
        "      print('Cannot find ', image_name)\n",
        "\n",
        "    h, w, c = image.shape\n",
        "    if aug:\n",
        "      ### scale the image, by 0 to 10%\n",
        "      scale = np.random.uniform()/10. + 1.  \n",
        "      img = cv2.resize(image, (0,0), fx = scale, fy = scale)\n",
        "\n",
        "      ### translate the image\n",
        "      max_offx = (scale-1.) * w\n",
        "      max_offy = (scale-1.) * h\n",
        "      offx = int(np.random.uniform() * max_offx)\n",
        "      offy = int(np.random.uniform() * max_offy)\n",
        "            \n",
        "      img = img[offy : (offy + h), offx : (offx + w)]\n",
        "      img/= 255\n",
        "      # image = self.aug_pipe.augment_image(image)\n",
        "\n",
        "    # resize the image to standard size and convert color\n",
        "    img = cv2.resize(img, (self.config['IMAGE_H'], self.config['IMAGE_W']))\n",
        "    img = img[:,:,::-1]\n",
        "\n",
        "    all_objs = copy.deepcopy(train_instance['object'])  # dict of all bbox coords and labels in an image\n",
        "    for obj in all_objs:\n",
        "\n",
        "      for attr in ['xmin', 'xmax']:\n",
        "        if aug: obj[attr] = int(obj[attr] * scale - offx)  # fix scaling and translation\n",
        "        obj[attr] = int(obj[attr] * float(self.config['IMAGE_W']) / w)  # resize annotations\n",
        "        obj[attr] = max(min(obj[attr], self.config['IMAGE_W']), 0)      # take care of boundary conditions \n",
        "\n",
        "      for attr in ['ymin', 'ymax']:\n",
        "        if aug: obj[attr] = int(obj[attr] * scale - offy)  # fix scaling and translation\n",
        "        obj[attr] = int(obj[attr] * float(self.config['IMAGE_H']) / h)  # resize annotations\n",
        "        obj[attr] = max(min(obj[attr], self.config['IMAGE_H']), 0)      # take care of boundary conditions\n",
        "\n",
        "    return img, all_objs       \n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    '''\n",
        "    input-\n",
        "        idx- non-negative integer\n",
        "    outputs-\n",
        "        x_batch: The numpy array of shape  (BATCH_SIZE, IMAGE_H, IMAGE_W, 3).\n",
        "            \n",
        "        y_batch: The numpy array of shape  (BATCH_SIZE, GRID_H, GRID_W, N_ANCHORS, 4 + 1). \n",
        "          \n",
        "          y_batch[iframe, igrid_h, igrid_w, ianchor, :4] contains (center_x, center_y, center_w, center_h) \n",
        "          of object if the object exists in this (grid cell, anchor) pair, else  0.\n",
        "          y_batch[iframe,igrid_h,igrid_w,ianchor,4] contains 1 if the object exists in this \n",
        "          (grid cell, anchor) pair, else 0.\n",
        "\n",
        "        b_batch: The numpy array of shape (BATCH_SIZE, 1, 1, 1, TRUE_BOX_BUFFER, 4).\n",
        "\n",
        "          b_batch[iframe, 1, 1, 1, ibuffer, :] contains ibufferth object's \n",
        "          (center_x, center_y, center_w, center_h) in iframeth frame.\n",
        "          If ibuffer > N objects in iframeth frame, then the values are simply 0.\n",
        "          This is just a hack to easily calculate loss. \n",
        "    ''' \n",
        "\n",
        "    l_bound = idx*self.config['BATCH_SIZE']\n",
        "    r_bound = (idx+1)*self.config['BATCH_SIZE']\n",
        "\n",
        "    if r_bound > len(self.images):\n",
        "      r_bound = len(self.images)\n",
        "      l_bound = r_bound - self.config['BATCH_SIZE']\n",
        "\n",
        "    instance_count = 0\n",
        "    ## prepare empty storage space: this will be output\n",
        "    x_batch = np.zeros((r_bound - l_bound, self.config['IMAGE_H'], self.config['IMAGE_W'], 3))                       # input images\n",
        "    b_batch = np.zeros((r_bound - l_bound, 1     , 1     , 1    ,  self.config['TRUE_BOX_BUFFER'], 4))               # list of self.config['TRUE_BOX_BUFFER'] GT boxes\n",
        "    y_batch = np.zeros((r_bound - l_bound, self.config['GRID_H'],  self.config['GRID_W'], self.config['N_ANCHORS'], 4+1)) # desired network output\n",
        "\n",
        "    for train_instance in self.images[l_bound: r_bound]:\n",
        "      img, all_objs = self.aug_image(train_instance, aug = self.aug)\n",
        "      true_box_index = 0\n",
        "\n",
        "      for obj in all_objs:\n",
        "        if obj['xmax'] > obj['xmin'] and obj['ymax'] > obj['ymin']:\n",
        "          center_x = .5*(obj['xmin'] + obj['xmax'])\n",
        "          center_x = center_x / (float(self.config['IMAGE_W']) / self.config['GRID_W'])                      # unit: grid cell\n",
        "          center_y = .5*(obj['ymin'] + obj['ymax'])                                                          \n",
        "          center_y = center_y / (float(self.config['IMAGE_H']) / self.config['GRID_H'])                      # unit: grid cell\n",
        "\n",
        "          grid_x = int(np.floor(center_x))\n",
        "          grid_y = int(np.floor(center_y))\n",
        "\n",
        "          if grid_x < self.config['GRID_W'] and grid_y < self.config['GRID_H']:\n",
        "            center_w = (obj['xmax'] - obj['xmin']) / (float(self.config['IMAGE_W']) / self.config['GRID_W']) # unit: grid cell\n",
        "            center_h = (obj['ymax'] - obj['ymin']) / (float(self.config['IMAGE_H']) / self.config['GRID_H']) # unit: grid cell\n",
        "            box = [center_x, center_y, center_w, center_h]\n",
        "\n",
        "            # find the anchor that best predicts this box\n",
        "            best_anchor = -1\n",
        "            max_iou     = -1\n",
        "            shifted_box = BoundBox(0,0,center_w, center_h)\n",
        "\n",
        "            for i in range(len(self.anchors)):\n",
        "              anchor = self.anchors[i]\n",
        "              iou = box_iou(shifted_box, anchor)  # GLOBAL FUNCTION\n",
        "\n",
        "              if max_iou<iou:\n",
        "                best_anchor = i\n",
        "                max_iou = iou\n",
        "\n",
        "            # assign ground truth x, y, w, h, confidence and class probs to y_batch\n",
        "            y_batch[instance_count, grid_y, grid_x, best_anchor, 0:4] = box\n",
        "            y_batch[instance_count, grid_y, grid_x, best_anchor, 4  ] = 1.\n",
        "                        \n",
        "            # assign the true box to b_batch\n",
        "            b_batch[instance_count, 0, 0, 0, true_box_index] = box\n",
        "                        \n",
        "            true_box_index += 1\n",
        "            true_box_index = true_box_index % self.config['TRUE_BOX_BUFFER']\n",
        "      # image assignment to x_batch\n",
        "      x_batch[instance_count] = img\n",
        "      # plt.imshow(img)\n",
        "      instance_count += 1\n",
        "\n",
        "    return [x_batch, b_batch], y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyxctFDtA9kC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def checkgen():\n",
        "  md = copy.copy(val_images[18:54])\n",
        "  bg = NEW(md, config_1) \n",
        "  bg.__len__()\n",
        "  [xxx,bbb], yyy = bg.__getitem__(0)\n",
        "  plt.imshow(xxx[0])\n",
        "  lbl = yyy[0]\n",
        "  mi = bg.load_image(0)\n",
        "  mi = cv2.resize(mi, (416,416))\n",
        "  plt.imshow(mi)\n",
        "\n",
        "  for h in range(lbl.shape[0]):\n",
        "    for w in range(lbl.shape[1]):\n",
        "      for an in range(lbl.shape[2]):\n",
        "        if lbl[h,w,an,4] == 1:\n",
        "          print(lbl[h,w,an,:4], '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7SIvtlvoVXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the parameters, global variables to be used \n",
        "def initialize():\n",
        "  IMAGE_DIMS = (416, 416)\n",
        "  IMAGE_W = IMAGE_DIMS[1]\n",
        "  IMAGE_H = IMAGE_DIMS[0]\n",
        "\n",
        "  GRID_DIMS = (32,32)\n",
        "  GRID_WSIZE = GRID_DIMS[1]\n",
        "  GRID_HSIZE = GRID_DIMS[0]\n",
        "\n",
        "  GRID_W = IMAGE_W//GRID_WSIZE\n",
        "  GRID_H = IMAGE_H//GRID_HSIZE\n",
        "  ANCHORS = np.load('/content/drive/My Drive/SVT/anchors_.npy')\n",
        "  ANCHORS = ANCHORS.reshape(10)\n",
        "  N_ANCHORS = len(ANCHORS)//2\n",
        "  TRUE_BOX_BUFFER = 50\n",
        "  LAMBDA_COORD = 1\n",
        "  LAMBDA_NO_OBJECT = 1\n",
        "  LAMBDA_OBJECT = 5\n",
        "  BATCH_SIZE = 4\n",
        "  config_dict = {\n",
        "    'IMAGE_W'         : IMAGE_W, \n",
        "    'IMAGE_H'         : IMAGE_H,\n",
        "    'GRID_WSIZE'      : GRID_WSIZE,\n",
        "    'GRID_HSIZE'      : GRID_HSIZE,\n",
        "    'GRID_W'          : GRID_W,  \n",
        "    'GRID_H'          : GRID_H,\n",
        "    'ANCHORS'         : ANCHORS,\n",
        "    'N_ANCHORS'       : N_ANCHORS,\n",
        "    'TRUE_BOX_BUFFER' : TRUE_BOX_BUFFER,\n",
        "    'LAMBDA_COORD'     : LAMBDA_COORD,\n",
        "    'LAMBDA_NO_OBJECT' : LAMBDA_NO_OBJECT,\n",
        "    'LAMBDA_OBJECT'    : LAMBDA_OBJECT,\n",
        "    'BATCH_SIZE'      : BATCH_SIZE\n",
        "  }\n",
        "\n",
        "  return(config_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul0K8gd_t_TF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # verification\n",
        "  # Mount google drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  import pickle \n",
        "  with open ('/content/drive/My Drive/SVT/val_images_.pickle', 'rb') as fp:\n",
        "    val_images = pickle.load(fp)\n",
        "    \n",
        "  config_1= initialize()\n",
        "  checkgen()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}